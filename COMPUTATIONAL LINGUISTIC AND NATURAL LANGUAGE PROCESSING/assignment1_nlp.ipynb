{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc28267",
   "metadata": {},
   "source": [
    "To classify text as humorous (1) or non-humorous (0) using various word embedding techniques and compare how these embeddings affect model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b0797b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f0ccc8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MSI\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\MSI\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df73b143",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  Why do Java developers wear glasses? Because t...      1\n",
      "1     I told my computer I needed a break… it froze.      1\n",
      "2  Debugging is like being the detective in a cri...      1\n",
      "3  Why did the neural network go to therapy? Too ...      1\n",
      "4       My data went on a date… now it’s an outlier.      1\n",
      "label\n",
      "0    31\n",
      "1    30\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and Explore Dataset\n",
    "df = pd.read_csv(\"humor_dataset.csv\")\n",
    "print(df.head())\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "728b86fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning & Preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{string.punctuation}]\", \"\", text)\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    tokens = text.split()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_text'] = df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28d588fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding Techniques\n",
    "\n",
    "# (a) One Hot Encoding\n",
    "vectorizer = CountVectorizer(binary=True)\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# (b) Bag of Words (BoW)\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# (c) Bag of N-grams (BoN)\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(df['clean_text'])\n",
    "\n",
    "# (d) TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ac7efbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (e) Word2Vec (CBOW & Skip-Gram)\\\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [row.split() for row in df['clean_text']]\n",
    "w2v_cbow = Word2Vec(sentences, vector_size=100, sg=0, min_count=1)\n",
    "w2v_skip = Word2Vec(sentences, vector_size=100, sg=1, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1408d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to sentence vectors (mean of word vectors):\n",
    "def get_sentence_vector(model, sentence):\n",
    "    words = [w for w in sentence.split() if w in model.wv]\n",
    "    if len(words)==0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(model.wv[words], axis=0)\n",
    "\n",
    "X_cbow = np.array([get_sentence_vector(w2v_cbow, t) for t in df['clean_text']])\n",
    "X_skip = np.array([get_sentence_vector(w2v_skip, t) for t in df['clean_text']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "107ec95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (f) fastText\n",
    "from gensim.models import FastText\n",
    "ft_model = FastText(sentences, vector_size=100, window=3, min_count=1)\n",
    "X_fast = np.array([get_sentence_vector(ft_model, t) for t in df['clean_text']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c41027cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2Vec (DM) Shape: (61, 100)\n",
      "Doc2Vec (DBOW) Shape: (61, 100)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "# Prepare tagged documents \n",
    "tagged_data = [TaggedDocument(words=text.split(), tags=[str(i)]) for i, text in enumerate(df['clean_text'])]\n",
    "\n",
    "# -------- Distributed Memory (DM) --------\n",
    "model_dm = Doc2Vec(\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    dm=1,\n",
    "    epochs=40,\n",
    "    workers=4\n",
    ")\n",
    "model_dm.build_vocab(tagged_data)\n",
    "model_dm.train(tagged_data, total_examples=model_dm.corpus_count, epochs=model_dm.epochs)\n",
    "\n",
    "# -------- Distributed Bag of Words (DBOW) --------\n",
    "model_dbow = Doc2Vec(\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    dm=0,\n",
    "    epochs=40,\n",
    "    workers=4\n",
    ")\n",
    "model_dbow.build_vocab(tagged_data)\n",
    "model_dbow.train(tagged_data, total_examples=model_dbow.corpus_count, epochs=model_dbow.epochs)\n",
    "\n",
    "# -------- Convert each sentence to vector --------\n",
    "X_dm = np.array([model_dm.dv[str(i)] for i in range(len(tagged_data))])\n",
    "X_dbow = np.array([model_dbow.dv[str(i)] for i in range(len(tagged_data))])\n",
    "\n",
    "print(\"Doc2Vec (DM) Shape:\", X_dm.shape)\n",
    "print(\"Doc2Vec (DBOW) Shape:\", X_dbow.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74e0aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training & Evaluation\n",
    "def evaluate_model(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6c4944e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Embedding  Accuracy\n",
      "0            One Hot  0.769231\n",
      "1                BoW  0.769231\n",
      "2                BoN  0.769231\n",
      "3             TF-IDF  0.692308\n",
      "4      Word2Vec CBOW  0.384615\n",
      "5  Word2Vec SkipGram  0.384615\n",
      "6           fastText  0.384615\n",
      "7         Doc2Vec DM  0.384615\n",
      "8       Doc2Vec DBOW  0.384615\n"
     ]
    }
   ],
   "source": [
    "y = df['label']\n",
    "\n",
    "results = {\n",
    "    \"One Hot\": evaluate_model(CountVectorizer(binary=True).fit_transform(df['clean_text']), y),\n",
    "    \"BoW\": evaluate_model(CountVectorizer().fit_transform(df['clean_text']), y),\n",
    "    \"BoN\": evaluate_model(CountVectorizer(ngram_range=(1,2)).fit_transform(df['clean_text']), y),\n",
    "    \"TF-IDF\": evaluate_model(TfidfVectorizer().fit_transform(df['clean_text']), y),\n",
    "    \"Word2Vec CBOW\": evaluate_model(X_cbow, y),\n",
    "    \"Word2Vec SkipGram\": evaluate_model(X_skip, y),\n",
    "    \"fastText\": evaluate_model(X_fast, y),\n",
    "    \"Doc2Vec DM\": evaluate_model(X_dm, y),\n",
    "    \"Doc2Vec DBOW\": evaluate_model(X_dbow, y)\n",
    "}\n",
    "\n",
    "print(pd.DataFrame(results.items(), columns=[\"Embedding\", \"Accuracy\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ee1b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
