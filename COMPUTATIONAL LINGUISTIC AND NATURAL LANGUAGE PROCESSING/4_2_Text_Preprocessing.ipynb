{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kScvafdaJPnb"
      },
      "source": [
        "#2. TEXT PREPROCESSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lez3CoMtJfwa"
      },
      "source": [
        "#a. BASIC CLEANING\n",
        "\n",
        "**i. HTML Tag Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Bx-E7ekSJO8D"
      },
      "outputs": [],
      "source": [
        "text1 = \"<p>The <b>movie</b> was <i>fantastic</i> and <a href='#'>enjoyable</a>.</p>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Fyokll7JKK-t"
      },
      "outputs": [],
      "source": [
        "import re #using RegEx\n",
        "def remove_html_tags(text):\n",
        "    pattern=re.compile('<.*?>')\n",
        "    return pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "iZx2nzNvKir3",
        "outputId": "193b1074-e16a-44bf-cdd2-c6305e5e0e9d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'The movie was fantastic and enjoyable.'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_html_tags(text1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTw2VxQdLC51"
      },
      "source": [
        "**ii. URL Removal**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Lm1QRbAsKop7"
      },
      "outputs": [],
      "source": [
        "text2=\"Check out the data at https://www.kaggle.com/code/anushkashukla1/notebooka7af49852a/edit\"\n",
        "text3=\"Google search here at www.google.com\"\n",
        "text4=\"For notebook click https://github.com/theinfoflux/Real-Time-Object-Detection-with-ESP32-CAM-OpenCV to search www.google.com\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "1aAnto0HK8iO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def remove_url(text):\n",
        "    pattern=re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    return pattern.sub(r'', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "38R1e34PK_0C",
        "outputId": "9fd3d373-4d8f-46a5-df21-e5b644822f9f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'For notebook click  to search '"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_url(text4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqZB_CofLO0F"
      },
      "source": [
        "**iii. Remove Punctuation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "QqIprg4ELJA7",
        "outputId": "40309cb3-a963-4341-af97-335f49aa06bb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import string\n",
        "string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d6LYparPLRtu"
      },
      "outputs": [],
      "source": [
        "exclude=string.punctuation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "trgIK6MQLT4q"
      },
      "outputs": [],
      "source": [
        "def remove_punc(text):\n",
        "    for char in exclude:\n",
        "        text=text.replace(char,'')\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PsA72SUNLVNL"
      },
      "outputs": [],
      "source": [
        "text5='remove. with. Punctuation?'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vuaUHd4La5L",
        "outputId": "7aa27d4a-7943-44ca-eda6-461a6617863b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "remove with Punctuation\n"
          ]
        }
      ],
      "source": [
        "print(remove_punc(text5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7TVW-YxNbC2"
      },
      "source": [
        "**iv. Basic Spell Check**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "X7X0V67yNING"
      },
      "outputs": [],
      "source": [
        "from textblob import TextBlob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "h-LoQSbyNfGE",
        "outputId": "a5db0616-2b04-4646-9099-cef4df7abfa1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'this is not justified, since it all be considered as a mistake'"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "incorrect_text='thhis is noot justifid, since it wll be considred as a misake'\n",
        "textBlb=TextBlob(incorrect_text)\n",
        "textBlb.correct().string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ynn7yBKZNk5b"
      },
      "source": [
        "**v. Handling Emojis**: Unicode Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TE1ebWYiNgZD"
      },
      "outputs": [],
      "source": [
        "emoji_text=\"Hey this is to convert emoji into a machine understandable format. Here is the üòä, please convert it.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "UGOx5JTZPZXf",
        "outputId": "2f3c4483-a57a-484a-abd6-f686112aceee"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Hey this is to convert emoji into a machine understandable format. Here is the üòä, please convert it.'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emoji_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxBLtHjaPfDG",
        "outputId": "0dcc485a-b5ca-4286-b960-e933fabfce8b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "b'Hey this is to convert emoji into a machine understandable format. Here is the \\xf0\\x9f\\x98\\x8a, please convert it.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "emoji_text.encode(\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VPBT62_EPvLC"
      },
      "source": [
        "#b. BASIC PREPROCESSING\n",
        "\n",
        "**i. Lowercasing**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TCN0Ag8XhQ2v"
      },
      "outputs": [],
      "source": [
        "text_clean = \"The Movie Was Absolutely Fantastic and Enjoyable! & the movie was a great experience.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N4ybdjGxhZLb"
      },
      "outputs": [],
      "source": [
        "def lowercase_text(text):\n",
        "    return text.lower()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "fGue87wThaqg",
        "outputId": "d4566683-05df-4371-b068-ba23aef2f442"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'the movie was absolutely fantastic and enjoyable! & the movie was a great experience.'"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "lowercase_text(text_clean)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfPgFDQ8hGlt"
      },
      "source": [
        "**ii. Tokenization**\n",
        "\n",
        "\n",
        "1.   Split function\n",
        "2.   RegEx\n",
        "3.   NLTK\n",
        "\n",
        "     i.   Word Tokenization\n",
        "\n",
        "     ii.   Sentence Tokenization\n",
        "4. SPACY (best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ix82ijUdaGg"
      },
      "source": [
        "**1. Split Function**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNp0BOLJdgXw",
        "outputId": "fa12f7c5-cca6-4c10-8220-8abd33b1cd5f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey', 'I', 'am', 'going', 'to', 'Delhi']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#word tokenization\n",
        "sent1='Hey I am going to Delhi'\n",
        "sent1.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ht0DHF6AdvWY",
        "outputId": "266ef86e-a7dd-451d-b1a6-520b67fef03e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey I am new to this', ' Will you please help me out', ' How to do it?']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#sent tokenization\n",
        "sent2=\"Hey I am new to this. Will you please help me out. How to do it?\"\n",
        "sent2.split('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oulbAsNYdybt",
        "outputId": "20a5862b-b66c-4cff-fdab-8f88833e2636"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey', ',', 'New', 'Delhi', 'is', 'the', 'capital', 'of', 'India!']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#problems with slpit function\n",
        "sent3=\"Hey , New Delhi is the capital of India!\"\n",
        "sent3.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWDC73ZHd38H",
        "outputId": "8959e205-5f04-470a-dc05-908bf4fa62ad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Hey can i have this? I do create one']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "#problem\n",
        "sent4=\"Hey can i have this? I do create one\"\n",
        "sent4.split('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yawtkMvKd5Qy"
      },
      "source": [
        "**2. RegEx**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XrG2TbZd8rW",
        "outputId": "e45dc58f-70b3-418f-b98a-af077a22f240"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
            "<>:3: SyntaxWarning: invalid escape sequence '\\w'\n",
            "C:\\Users\\presh\\AppData\\Local\\Temp\\ipykernel_8488\\886222352.py:3: SyntaxWarning: invalid escape sequence '\\w'\n",
            "  tokens=re.findall(\"[\\w']+\", sent5) #problem since we can't find the !, for that we need to find out different patterns\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'Delhi']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "sent5=\"I am going to Delhi!\"\n",
        "tokens=re.findall(\"[\\w']+\", sent5) #problem since we can't find the !, for that we need to find out different patterns\n",
        "tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PWsucV-ueAIV",
        "outputId": "dabcd083-6125-4e86-c0f4-3b89dd7e1080"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Chat abbreviations and slang have become a common part of online communication.\\nPeople use them to type faster and convey emotions or responses more efficiently.\\nFor example, instead of writing ‚ÄúBe Right Back,‚Äù users often type ‚ÄúBRB.‚Äù\\nThese short forms are especially popular in text messaging, social media, and gaming platforms?\\nLearning these abbreviations can help you understand conversations better and engage more naturally\\nin digital interactions',\n",
              " \"Whether you're chatting with friends or participating in online forums,\\nknowing these terms can make communication quicker and more fun \"]"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text=   \"\"\"Chat abbreviations and slang have become a common part of online communication.\n",
        "People use them to type faster and convey emotions or responses more efficiently.\n",
        "For example, instead of writing ‚ÄúBe Right Back,‚Äù users often type ‚ÄúBRB.‚Äù\n",
        "These short forms are especially popular in text messaging, social media, and gaming platforms?\n",
        "Learning these abbreviations can help you understand conversations better and engage more naturally\n",
        "in digital interactions. Whether you're chatting with friends or participating in online forums,\n",
        "knowing these terms can make communication quicker and more fun \"\"\"\n",
        "\n",
        "sentences=re.compile('[.?\\\"] ').split(text)\n",
        "sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GjGrbHsndhEY"
      },
      "source": [
        "**3. NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "C1p1_0GMQm5H"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize, sent_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxmmtrfXQs0Z",
        "outputId": "6dfc5e8e-4cc8-48f8-efda-492559c86712"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'am', 'going', 'to', 'Delhi', '!']"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#word tokenization\n",
        "sent1=\"I am going to Delhi!\"\n",
        "word_tokenize(sent1)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OARIsZgdQ3S2"
      },
      "outputs": [],
      "source": [
        "sent6=\"I have a Ph.D in A.I.\"\n",
        "sent7=\"We're here to help, please mail us @www.google.com\"\n",
        "sent8=\"A 5km ride cost $58.7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3neOUcQ6PgxP",
        "outputId": "8df69ce1-bc4a-4c3a-f049-3b03782dd162"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['I', 'have', 'a', 'Ph.D', 'in', 'A.I', '.']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(sent6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4IMDebb3ei1s",
        "outputId": "f8b14717-6e76-4fa5-90eb-fa0cba38080b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['We',\n",
              " \"'re\",\n",
              " 'here',\n",
              " 'to',\n",
              " 'help',\n",
              " ',',\n",
              " 'please',\n",
              " 'mail',\n",
              " 'us',\n",
              " '@',\n",
              " 'www.google.com']"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(sent7) #some prblm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2fw4F6wemab",
        "outputId": "59ad3a4e-7beb-4d97-88c8-f845a755ed15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['A', '5km', 'ride', 'cost', '$', '58.7']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "word_tokenize(sent8) #some prblm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "d9tVh6IGeo06"
      },
      "outputs": [],
      "source": [
        "dummy= \"Python is one of the most popular programming languages. It‚Äôs simple to use, packed with features and supported by a wide range of libraries and frameworks. Its clean syntax makes it beginner-friendly.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Y5wYdW55fLwd",
        "outputId": "6a2d73c6-b930-4452-90a3-8ea205ee41aa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Python is one of the most popular programming languages. It‚Äôs simple to use, packed with features and supported by a wide range of libraries and frameworks. Its clean syntax makes it beginner-friendly.'"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dummy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ac3U-u9KarU7",
        "outputId": "afb32a06-ae49-4d41-f306-3dfd7127209f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Python is one of the most popular programming languages.', 'It‚Äôs simple to use, packed with features and supported by a wide range of libraries and frameworks.', 'Its clean syntax makes it beginner-friendly.']\n"
          ]
        }
      ],
      "source": [
        "#sentence tokenization\n",
        "sents = sent_tokenize(dummy)\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS5jeUQVeJiW"
      },
      "source": [
        "**4. SPACY**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LzKpkeajeM-w",
        "outputId": "23b806f0-9948-48ea-f214-48efedc86c77"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspacy\u001b[39;00m\n\u001b[32m      2\u001b[39m nlp=spacy.load(\u001b[33m'\u001b[39m\u001b[33men_core_web_sm\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp=spacy.load('en_core_web_sm')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cf9RoDzqfX7s"
      },
      "outputs": [],
      "source": [
        "doc1=nlp(sent6)\n",
        "doc2=nlp(sent7)\n",
        "doc3=nlp(sent8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dmd_pLV5fa1X",
        "outputId": "9e73635d-132d-47df-f01d-b66f23988b11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A\n",
            "5\n",
            "km\n",
            "ride\n",
            "cost\n",
            "$\n",
            "58.7\n"
          ]
        }
      ],
      "source": [
        "for token in doc3: #performing a bit better than nltk\n",
        "    print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TCRO3uvbO78"
      },
      "source": [
        "**iii. Stop Words Removal**\n",
        "\n",
        "Stopwords: very common words that carry little meaning on their own."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Why we remove them\n",
        "\n",
        "1. They appear everywhere and drown out the important words.\n",
        "2. Fewer words means faster models and smaller features.\n",
        "3. It reduces noise for tasks like search and topic modeling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dc91f13",
        "outputId": "b1388887-995f-457b-a076-0c82e832f8d5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Types of Stopwords\n",
        "\n",
        "1. Common stopwords: Super-common words that add little meaning.\n",
        "\n",
        "Examples: the, is, in, to, at.\n",
        "\n",
        "2. Custom stopwords: Extra words you choose to ignore for your task.\n",
        "\n",
        "Example (medical notes): patient, treatment.\n",
        "\n",
        "3. Numerical stopwords: Numbers you drop when numbers don‚Äôt matter.\n",
        "\n",
        "Example: 2021, 3, 50%.\n",
        "\n",
        "4. Single-character stopwords: One-letter tokens that aren‚Äôt useful.\n",
        "\n",
        "Example: a, I, x.\n",
        "\n",
        "5. Contextual stopwords: Words you drop in some tasks but keep in others.\n",
        "\n",
        "Example: ‚Äúwill‚Äù is often noise, but matters for future-tense prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DiF-JeGbDvA"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mkyxbJ2biyr",
        "outputId": "892f3e82-5d43-41ff-e0e1-81da2a265ef7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f214c9cb",
        "outputId": "c77d2232-2339-43c6-fb0a-fd2210e5e66a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['au',\n",
              " 'aux',\n",
              " 'avec',\n",
              " 'ce',\n",
              " 'ces',\n",
              " 'dans',\n",
              " 'de',\n",
              " 'des',\n",
              " 'du',\n",
              " 'elle',\n",
              " 'en',\n",
              " 'et',\n",
              " 'eux',\n",
              " 'il',\n",
              " 'ils',\n",
              " 'je',\n",
              " 'la',\n",
              " 'le',\n",
              " 'les',\n",
              " 'leur',\n",
              " 'lui',\n",
              " 'ma',\n",
              " 'mais',\n",
              " 'me',\n",
              " 'm√™me',\n",
              " 'mes',\n",
              " 'moi',\n",
              " 'mon',\n",
              " 'ne',\n",
              " 'nos',\n",
              " 'notre',\n",
              " 'nous',\n",
              " 'on',\n",
              " 'ou',\n",
              " 'par',\n",
              " 'pas',\n",
              " 'pour',\n",
              " 'qu',\n",
              " 'que',\n",
              " 'qui',\n",
              " 'sa',\n",
              " 'se',\n",
              " 'ses',\n",
              " 'son',\n",
              " 'sur',\n",
              " 'ta',\n",
              " 'te',\n",
              " 'tes',\n",
              " 'toi',\n",
              " 'ton',\n",
              " 'tu',\n",
              " 'un',\n",
              " 'une',\n",
              " 'vos',\n",
              " 'votre',\n",
              " 'vous',\n",
              " 'c',\n",
              " 'd',\n",
              " 'j',\n",
              " 'l',\n",
              " '√†',\n",
              " 'm',\n",
              " 'n',\n",
              " 's',\n",
              " 't',\n",
              " 'y',\n",
              " '√©t√©',\n",
              " '√©t√©e',\n",
              " '√©t√©es',\n",
              " '√©t√©s',\n",
              " '√©tant',\n",
              " '√©tante',\n",
              " '√©tants',\n",
              " '√©tantes',\n",
              " 'suis',\n",
              " 'es',\n",
              " 'est',\n",
              " 'sommes',\n",
              " '√™tes',\n",
              " 'sont',\n",
              " 'serai',\n",
              " 'seras',\n",
              " 'sera',\n",
              " 'serons',\n",
              " 'serez',\n",
              " 'seront',\n",
              " 'serais',\n",
              " 'serait',\n",
              " 'serions',\n",
              " 'seriez',\n",
              " 'seraient',\n",
              " '√©tais',\n",
              " '√©tait',\n",
              " '√©tions',\n",
              " '√©tiez',\n",
              " '√©taient',\n",
              " 'fus',\n",
              " 'fut',\n",
              " 'f√ªmes',\n",
              " 'f√ªtes',\n",
              " 'furent',\n",
              " 'sois',\n",
              " 'soit',\n",
              " 'soyons',\n",
              " 'soyez',\n",
              " 'soient',\n",
              " 'fusse',\n",
              " 'fusses',\n",
              " 'f√ªt',\n",
              " 'fussions',\n",
              " 'fussiez',\n",
              " 'fussent',\n",
              " 'ayant',\n",
              " 'ayante',\n",
              " 'ayantes',\n",
              " 'ayants',\n",
              " 'eu',\n",
              " 'eue',\n",
              " 'eues',\n",
              " 'eus',\n",
              " 'ai',\n",
              " 'as',\n",
              " 'avons',\n",
              " 'avez',\n",
              " 'ont',\n",
              " 'aurai',\n",
              " 'auras',\n",
              " 'aura',\n",
              " 'aurons',\n",
              " 'aurez',\n",
              " 'auront',\n",
              " 'aurais',\n",
              " 'aurait',\n",
              " 'aurions',\n",
              " 'auriez',\n",
              " 'auraient',\n",
              " 'avais',\n",
              " 'avait',\n",
              " 'avions',\n",
              " 'aviez',\n",
              " 'avaient',\n",
              " 'eut',\n",
              " 'e√ªmes',\n",
              " 'e√ªtes',\n",
              " 'eurent',\n",
              " 'aie',\n",
              " 'aies',\n",
              " 'ait',\n",
              " 'ayons',\n",
              " 'ayez',\n",
              " 'aient',\n",
              " 'eusse',\n",
              " 'eusses',\n",
              " 'e√ªt',\n",
              " 'eussions',\n",
              " 'eussiez',\n",
              " 'eussent']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopwords.words('french')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7nk7HIiDb6Ig"
      },
      "outputs": [],
      "source": [
        "def remove_stopwords(text):\n",
        "    new_text=[]\n",
        "\n",
        "    for word in text.split():\n",
        "        if word in stopwords.words('english'):\n",
        "            new_text.append(' ')\n",
        "        else:\n",
        "            new_text.append(word)\n",
        "    x=new_text[:]\n",
        "    new_text.clear()\n",
        "    return \" \".join(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KPEdA8ofb-TD",
        "outputId": "7834a3fd-346b-4303-add2-8e441d7f7930"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'        make students learn     class   things   mandatory   one    '"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "remove_stopwords('this is how we make students learn in the class these things are mandatory for one to have')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H87yf41WcMvk"
      },
      "source": [
        "**iv. Chat Conversation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSXIAx6HcAbh"
      },
      "outputs": [],
      "source": [
        "chat_words = {\n",
        "    \"AFAIK\": \"As Far As I Know\",\n",
        "    \"AFK\": \"Away From Keyboard\",\n",
        "    \"ASAP\": \"As Soon As Possible\",\n",
        "    \"ATK\": \"At The Keyboard\",\n",
        "    \"ATM\": \"At The Moment\",\n",
        "    \"A3\": \"Anytime, Anywhere, Anyplace\",\n",
        "    \"BAK\": \"Back At Keyboard\",\n",
        "    \"BBL\": \"Be Back Later\",\n",
        "    \"BBS\": \"Be Back Soon\",\n",
        "    \"BFN\": \"Bye For Now\",\n",
        "    \"B4N\": \"Bye For Now\",\n",
        "    \"BRB\": \"Be Right Back\",\n",
        "    \"BRT\": \"Be Right There\",\n",
        "    \"BTW\": \"By The Way\",\n",
        "    \"B4\": \"Before\",\n",
        "    \"CU\": \"See You\",\n",
        "    \"CUL8R\": \"See You Later\",\n",
        "    \"CYA\": \"See You\",\n",
        "    \"FAQ\": \"Frequently Asked Questions\",\n",
        "    \"FC\": \"Fingers Crossed\",\n",
        "    \"FWIW\": \"For What It's Worth\",\n",
        "    \"FYI\": \"For Your Information\",\n",
        "    \"GAL\": \"Get A Life\",\n",
        "    \"GG\": \"Good Game\",\n",
        "    \"GN\": \"Good Night\",\n",
        "    \"GMTA\": \"Great Minds Think Alike\",\n",
        "    \"GR8\": \"Great!\",\n",
        "    \"G9\": \"Genius\",\n",
        "    \"IC\": \"I See\",\n",
        "    \"ICQ\": \"I Seek you (also a chat program)\",\n",
        "    \"ILU\": \"I Love You\",\n",
        "    \"IMHO\": \"In My Honest/Humble Opinion\",\n",
        "    \"IMO\": \"In My Opinion\",\n",
        "    \"IOW\": \"In Other Words\",\n",
        "    \"IRL\": \"In Real Life\",\n",
        "    \"KISS\": \"Keep It Simple, Stupid\",\n",
        "    \"LDR\": \"Long Distance Relationship\",\n",
        "    \"LMAO\": \"Laughing my a** off\",\n",
        "    \"LOL\": \"Laughing out loud\",\n",
        "    \"LTNS\": \"Long Time No See\",\n",
        "    \"L8R\": \"Later\",\n",
        "    \"MTE\": \"My Thoughts Exactly\",\n",
        "    \"M8\": \"Mate\",\n",
        "    \"NRN\": \"No Reply Necessary\",\n",
        "    \"OIC\": \"Oh I See\",\n",
        "    \"PITA\": \"Pain In The A..\",\n",
        "    \"PRT\": \"Party\",\n",
        "    \"PRW\": \"Parents Are Watching\",\n",
        "    \"QPSA?\": \"Que Pasa?\",\n",
        "    \"ROFL\": \"Rolling On The Floor Laughing\",\n",
        "    \"ROFLOL\": \"Rolling On The Floor Laughing Out Loud\",\n",
        "    \"ROTFLMAO\": \"Rolling On The Floor Laughing My A.. Off\",\n",
        "    \"SK8\": \"Skate\",\n",
        "    \"STATS\": \"Your sex and age\",\n",
        "    \"ASL\": \"Age, Sex, Location\",\n",
        "    \"THX\": \"Thank You\",\n",
        "    \"TTFN\": \"Ta-Ta For Now!\",\n",
        "    \"TTYL\": \"Talk To You Later\",\n",
        "    \"U\": \"You\",\n",
        "    \"U2\": \"You Too\",\n",
        "    \"U4E\": \"Yours For Ever\",\n",
        "    \"WB\": \"Welcome Back\",\n",
        "    \"WTF\": \"What The F...\",\n",
        "    \"WTG\": \"Way To Go!\",\n",
        "    \"WUF\": \"Where Are You From?\",\n",
        "    \"W8\": \"Wait...\",\n",
        "    \"7K\": \"Sick:-D Laugher\",\n",
        "    \"TFW\": \"That feeling when\",\n",
        "    \"MFW\": \"My face when\",\n",
        "    \"MRW\": \"My reaction when\",\n",
        "    \"IFYP\": \"I feel your pain\",\n",
        "    \"TNTL\": \"Trying not to laugh\",\n",
        "    \"JK\": \"Just kidding\",\n",
        "    \"IDC\": \"I don‚Äôt care\",\n",
        "    \"ILY\": \"I love you\",\n",
        "    \"IMU\": \"I miss you\",\n",
        "    \"ADIH\": \"Another day in hell\",\n",
        "    \"ZZZ\": \"Sleeping, bored, tired\",\n",
        "    \"WYWH\": \"Wish you were here\",\n",
        "    \"TIME\": \"Tears in my eyes\",\n",
        "    \"BAE\": \"Before anyone else\",\n",
        "    \"FIMH\": \"Forever in my heart\",\n",
        "    \"BSAAW\": \"Big smile and a wink\",\n",
        "    \"BWL\": \"Bursting with laughter\",\n",
        "    \"BFF\": \"Best friends forever\",\n",
        "    \"CSL\": \"Can‚Äôt stop laughing\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJQu0m3acSFQ"
      },
      "outputs": [],
      "source": [
        "def chat_conv(text):\n",
        "    new_text=[]\n",
        "    for w in text.split():\n",
        "        if w.upper() in chat_words:\n",
        "            new_text.append(chat_words[w.upper()])\n",
        "        else:\n",
        "            new_text.append(w)\n",
        "    return \" \".join(new_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "EVw6gd2CcUx-",
        "outputId": "3ac7977d-f035-4a48-f605-5845a13b30ff"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'As Far As I Know He is the best guy.'"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_conv(\"AFAIK He is the best guy.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3SV7F5C5ceSV"
      },
      "source": [
        "**v. Stemming/Lemmatization**\n",
        "\n",
        "Stemming: extracts the root of the word but not ensures that root word is a part of English language or dictionary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### a. Porter Stemmer\n",
        "Porter Stemmer is the original stemmer but the stem sometimes illogical or\n",
        "non-dictionary word.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wpUW0BzBccT8"
      },
      "outputs": [],
      "source": [
        "from nltk.stem.porter import PorterStemmer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWqfdA-NfkGS"
      },
      "outputs": [],
      "source": [
        "ps=PorterStemmer()\n",
        "def stem_words(text):\n",
        "    return \" \".join([ps.stem(word) for word in text.split()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6LQGLkofmwA"
      },
      "outputs": [],
      "source": [
        "sample=\"walking walked walks walk\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "kCvm-LCUfoO0",
        "outputId": "a871e65c-e0cc-4a81-e7e8-8d5ec015f894"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'walk walk walk walk'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stem_words(sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2gpIbdSfpsw",
        "outputId": "409334fc-72f8-490f-a0d4-061ab36c8a58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The children were playing in the garden while their mother was cooking. She enjoys gardening and often gardens on weekends. The players played with great enthusiasm, hoping to impress the coaches. Some were running, others had run earlier, and a few enjoyed the run thoroughly. He studies daily, studied last night, and is currently studying math. They happily agreed to the agreement after arguing about the argument.\n"
          ]
        }
      ],
      "source": [
        "text=\"The children were playing in the garden while their mother was cooking. She enjoys gardening and often gardens on weekends. The players played with great enthusiasm, hoping to impress the coaches. Some were running, others had run earlier, and a few enjoyed the run thoroughly. He studies daily, studied last night, and is currently studying math. They happily agreed to the agreement after arguing about the argument.\"\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yD7UghJNfrO2",
        "outputId": "a537f241-6ccf-4877-f725-89a2d2efb2d7"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'the children were play in the garden while their mother wa cooking. she enjoy garden and often garden on weekends. the player play with great enthusiasm, hope to impress the coaches. some were running, other had run earlier, and a few enjoy the run thoroughly. he studi daily, studi last night, and is current studi math. they happili agre to the agreement after argu about the argument.'"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stem_words(text) # some prblm is their since its giving the root word that is not excatly the root word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### b. Snowball Stemmer\n",
        "Snowball stemmer is faster and more logical than the Porter Stemmer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Drivin\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence after Porter Stemming: connect connect connect connect connect connect connect drivin\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "snowball=SnowballStemmer(language=\"english\")\n",
        "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Drivin\"\n",
        "snowball_stem=[snowball.stem(word) for word in sentence.split(\" \")]\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"--\"*60)\n",
        "print(\"Sentence after Porter Stemming:\", \" \".join(snowball_stem))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## c. Lancaster Stemmer\n",
        "The Lancaster stemmers are more aggressive and dynamic. The stemmer is\n",
        "really faster, but the algorithm is really confusing when dealing with small\n",
        "words. Lancaster Stemmer produces results with excessive stemming.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Drivin\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence after Porter Stemming: connect connect connect connect connect connect connect drivin\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "lancaster=LancasterStemmer()\n",
        "sentence= \"Connect Connection Connections Connecting Connected Connects Connectings Drivin\"\n",
        "lancaster_stem=[lancaster.stem(word) for word in sentence.split(\" \")]\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"--\"*60)\n",
        "print(\"Sentence after Porter Stemming:\", \" \".join(lancaster_stem))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## d. Regexp Stemmer\n",
        "Regexp stemmer identifies morphological affixes using regular expressions.\n",
        "Substrings matching the regular expressions will be discarded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence: Connect Connection Connections Connecting Connected Connects Connectings Drivin\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence after Porter Stemming: Connect Connection Connection Connect Connected Connect Connecting Drivin\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "regex=RegexpStemmer(regexp=\"ing$|s$|e$\", min=0)\n",
        "sentence=\"Connect Connection Connections Connecting Connected Connects Connectings Drivin\"\n",
        "regex_stem=[regex.stem(word) for word in sentence.split(\" \")]\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"--\"*60)\n",
        "print(\"Sentence after Porter Stemming:\", \" \".join(regex_stem))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lemmatization: \n",
        "Ensures the root word is a part of English language or dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86ed1f9e",
        "outputId": "f76a8328-76cf-4618-ff99-1246263c1beb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Wordnet Lemmitizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sOW9iReZfuCY",
        "outputId": "0fd853b9-726a-4da7-b622-1fde75953f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Word                 Lemma               \n",
            "He                   He                  \n",
            "was                  be                  \n",
            "running              run                 \n",
            "and                  and                 \n",
            "eating               eat                 \n",
            "at                   at                  \n",
            "same                 same                \n",
            "time                 time                \n",
            "He                   He                  \n",
            "has                  have                \n",
            "bad                  bad                 \n",
            "habit                habit               \n",
            "of                   of                  \n",
            "swimming             swim                \n",
            "after                after               \n",
            "playing              play                \n",
            "long                 long                \n",
            "hours                hours               \n",
            "in                   in                  \n",
            "the                  the                 \n",
            "Sun                  Sun                 \n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "wordnet_lemmatizer= WordNetLemmatizer()\n",
        "\n",
        "sentence=\"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun\"\n",
        "punctuations=\"?:!.,;\"\n",
        "sentence_words=nltk.word_tokenize(sentence)\n",
        "for word in sentence_words:\n",
        "    if word in punctuations:\n",
        "        sentence_words.remove(word)\n",
        "\n",
        "sentence_words\n",
        "print(\"{0:20} {1:20}\". format(\"Word\", \"Lemma\"))\n",
        "for word in sentence_words:\n",
        "    print(\"{0:20} {1:20}\".format(word, wordnet_lemmatizer.lemmatize(word, pos='v'))) #need to specify pos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TextBlob Lemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Sentence: The bats are hanging on their feet in upright positions\n",
            "------------------------------------------------------------------------------------------------------------------------\n",
            "Sentence after Lemmatization: The bat are hanging on their foot in upright position\n"
          ]
        }
      ],
      "source": [
        "from textblob import TextBlob\n",
        "sentence=\"The bats are hanging on their feet in upright positions\"\n",
        "sent=TextBlob(sentence)\n",
        "texblob_lemma=[w.lemmatize() for w in sent.words]\n",
        "print(\"Original Sentence:\", sentence)\n",
        "print(\"--\"*60)\n",
        "print(\"Sentence after Lemmatization:\", \" \".join(texblob_lemma))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTL6tilxh0ct"
      },
      "source": [
        "**vi. Language Detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-9DknT1iCJi",
        "outputId": "c7eb7ccb-1db0-4af3-ade2-4c170886078d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5OH5NmhgEvD"
      },
      "outputs": [],
      "source": [
        "text_clean2 = \"La pel√≠cula fue absolutamente fant√°stica y agradable\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6i3l3knh-E3"
      },
      "outputs": [],
      "source": [
        "from langdetect import detect, detect_langs\n",
        "\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        lang = detect(text)\n",
        "        lang_prob = detect_langs(text)\n",
        "        return lang, lang_prob\n",
        "    except Exception as e:\n",
        "        return \"Error\", str(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGCCZetFiFzL",
        "outputId": "4ff3e291-daa7-4aca-aa37-4b481e2198d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: La pel√≠cula fue absolutamente fant√°stica y agradable\n",
            "Detected Language Code: es\n",
            "Confidence: [es:0.999995932581196]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "text_clean2 = \"La pel√≠cula fue absolutamente fant√°stica y agradable\"\n",
        "lang_code, lang_probs = detect_language(text_clean2)\n",
        "\n",
        "print(\"Text:\", text_clean2)\n",
        "print(\"Detected Language Code:\", lang_code)\n",
        "print(\"Confidence:\", lang_probs) #es-Spanish"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yqfCulSiUUw"
      },
      "source": [
        "#c. ADVANCED PREPROCESSING\n",
        "\n",
        "**i. POS Tagging**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4M0PHYqviI0G"
      },
      "outputs": [],
      "source": [
        "text_adv1 = \"The quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ptpt9Ji5iiX4",
        "outputId": "5c0decdc-ab7c-4e99-ba83-969bde7e551a"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "def pos_tag_text(text):\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tagged = nltk.pos_tag(tokens)\n",
        "    return tagged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwtp51Q4ilaK",
        "outputId": "f01047f1-34c3-4434-adcf-9f09e8bb933c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: The quick brown fox jumps over the lazy dog\n",
            "POS Tags: [('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN')]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "text_adv1 = \"The quick brown fox jumps over the lazy dog\"\n",
        "pos_tags = pos_tag_text(text_adv1)\n",
        "\n",
        "print(\"Text:\", text_adv1)\n",
        "print(\"POS Tags:\", pos_tags) #DT-Determiner, JJ-Adjective, NN-Noun, VBZ-Verb (3rd person singular), IN-Preposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVDRmTXljMR0"
      },
      "source": [
        "**ii. Parsing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UB9lu2omjU20",
        "outputId": "da2efc42-bfe3-472c-f2f7-5266da5a98dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.7.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.11/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.7)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.7.14)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.21.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.3.0.post1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n",
            "Installing collected packages: en-core-web-sm\n",
            "  Attempting uninstall: en-core-web-sm\n",
            "    Found existing installation: en_core_web_sm 3.8.0\n",
            "    Uninstalling en_core_web_sm-3.8.0:\n",
            "      Successfully uninstalled en_core_web_sm-3.8.0\n",
            "Successfully installed en-core-web-sm-3.7.1\n",
            "\u001b[38;5;2m‚úî Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m‚ö† Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6ZPNNMfimgC"
      },
      "outputs": [],
      "source": [
        "text_adv2 = \"The quick brown fox jumps over the lazy dog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wSOFSSW8jRjS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "\n",
        "# Load English language model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def parse_sentence(text):\n",
        "    doc = nlp(text)\n",
        "    for token in doc:\n",
        "        print(f\"{token.text:<12} {token.dep_:<10} {token.head.text:<10} {token.pos_:<6}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R_GKjgyHjp7N",
        "outputId": "a63da599-7625-41c8-d9eb-0822f978721e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parsed Tokens:\n",
            "The          det        fox        DET   \n",
            "quick        amod       fox        ADJ   \n",
            "brown        amod       fox        ADJ   \n",
            "fox          nsubj      jumps      NOUN  \n",
            "jumps        ROOT       jumps      VERB  \n",
            "over         prep       jumps      ADP   \n",
            "the          det        dog        DET   \n",
            "lazy         amod       dog        ADJ   \n",
            "dog          pobj       over       NOUN  \n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "text_adv2 = \"The quick brown fox jumps over the lazy dog\"\n",
        "print(\"Parsed Tokens:\")\n",
        "parse_sentence(text_adv2) #nsubj: nominal subject of the verb, ROOT: main verb of the sentence, amod: adjective modifier, pobj: object of a preposition, det: determiner"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2vQQ-L7kEmh"
      },
      "source": [
        "**iii. Coreference Resolution**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8R6R4I_unLpT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv (3.13.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
